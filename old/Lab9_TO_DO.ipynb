{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b548c82",
   "metadata": {},
   "source": [
    "# Dogs vs Cats\n",
    "\n",
    "To classify whether images contain either a dog or a cat.\n",
    "\n",
    "**Link:** [https://www.kaggle.com/datasets/salader/dogs-vs-cats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2328e0",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd3259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensor computation with strong GPU acceleration and deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "# Using numpy\n",
    "import numpy as np\n",
    "\n",
    "# For data load or save\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For operating system dependent functionality\n",
    "import os,shutil\n",
    "\n",
    "# For k-folds cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# To unzip datasets\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# To generate pseudo-random numbers\n",
    "import random\n",
    "\n",
    "# Python Imaging Library (PIL) for added image processing capabilities \n",
    "from PIL import Image\n",
    "\n",
    "# To make loops show a progress meter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# To manage colorbar in subplots\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007bb798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:  ['.DS_Store', 'Lab9_TO_DO.ipynb', 'Lab9.pptx']\n"
     ]
    }
   ],
   "source": [
    "# Running this (by clicking run or pressing Shift+Enter) will list the files in the current directory\n",
    "print(\"Current directory: \",os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fef554d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'archive.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## These lines of code must be run only once after dataset download\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Unzipping the dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marchive.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      5\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Path of sources\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1331\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'archive.zip'"
     ]
    }
   ],
   "source": [
    "## These lines of code must be run only once after dataset download\n",
    "\n",
    "# Unzipping the dataset\n",
    "with ZipFile('archive.zip','r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Path of sources\n",
    "path_train_cats = \"train/cats/\"\n",
    "path_train_dogs = \"train/dogs/\"\n",
    "path_test_cats = \"test/cats/\"\n",
    "path_test_dogs = \"test/dogs/\"\n",
    "\n",
    "# Path of destination\n",
    "path_dst = \"dataset\"\n",
    "\n",
    "# Destination folder creation]\n",
    "os.mkdir(path_dst)\n",
    "\n",
    "# Iterate on all files to move them to destination folder\n",
    "# Cats from train dir\n",
    "allfiles = os.listdir(path_train_cats)\n",
    "for f in allfiles:\n",
    "    src = os.path.join(path_train_cats,f)\n",
    "    dst = os.path.join(path_dst,f)\n",
    "    shutil.move(src,dst)\n",
    "# Dogs from train dir\n",
    "allfiles = os.listdir(path_train_dogs)\n",
    "for f in allfiles:\n",
    "    src = os.path.join(path_train_dogs,f)\n",
    "    dst = os.path.join(path_dst,f)\n",
    "    shutil.move(src,dst)\n",
    "# Cats from test dir\n",
    "allfiles = os.listdir(path_test_cats)\n",
    "for f in allfiles:\n",
    "    src = os.path.join(path_test_cats,f)\n",
    "    dst = os.path.join(path_dst,f)\n",
    "    shutil.move(src,dst)\n",
    "# Dogs from test dir\n",
    "allfiles = os.listdir(path_test_dogs)\n",
    "for f in allfiles:\n",
    "    src = os.path.join(path_test_dogs,f)\n",
    "    dst = os.path.join(path_dst,f)\n",
    "    shutil.move(src,dst)\n",
    "    \n",
    "# Removing all unnecessary files\n",
    "shutil.rmtree(\"dogs_vs_cats\")\n",
    "shutil.rmtree(\"train\")\n",
    "shutil.rmtree(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists for dataframe construction\n",
    "allfiles = os.listdir(\"dataset\")\n",
    "\n",
    "# Extracting labels from filenames\n",
    "labels = []\n",
    "for filename in allfiles:\n",
    "    # 0 for 'cat', 1 for 'dog'\n",
    "    if filename.split(\".\")[0] == \"cat\":\n",
    "        labels.append(0)\n",
    "    elif filename.split(\".\")[0] == \"dog\":\n",
    "        labels.append(1)\n",
    "\n",
    "print(\"Images count = \",len(allfiles),\"\\nLabels count = \",len(labels))\n",
    "\n",
    "df = pd.DataFrame(columns=[\"filename\",\"label\"])\n",
    "df[\"filename\"] = allfiles\n",
    "df[\"label\"] = labels\n",
    "\n",
    "print(\"Some items from the dataframe just created:\\n\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot to visualize the distribution of the labels \n",
    "fig = plt.figure(figsize=(5,4))\n",
    "sns.set_style(\"white\")\n",
    "sns.countplot(data=df,x=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63452b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performing a stratified k-folds subdivision\n",
    "#skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "#skf.get_n_splits(df.filename,df.label)\n",
    "#\n",
    "## Storing indices to recover the splits in a second time\n",
    "#train_idx = []\n",
    "#val_idx = []\n",
    "#for i,(train,val) in enumerate(skf.split(df.filename,df.label)):\n",
    "#    train_idx.append(train)\n",
    "#    val_idx.append(val)\n",
    "#    print(\"Fold: \",i)\n",
    "#    print(\"\\nTrain: index = \",train)\n",
    "#    print(\"\\nValidation:  index = \",val,\"\\n\")\n",
    "#\n",
    "## Storing in csv files    \n",
    "#train_splits = pd.DataFrame(columns=[\"train_0\",\"train_1\",\"train_2\",\"train_3\",\"train_4\",\"train_5\",\"train_6\",\"train_7\",\"train_8\",\"train_9\"])\n",
    "#val_splits = pd.DataFrame(columns=[\"val_0\",\"val_1\",\"val_2\",\"val_3\",\"val_4\",\"val_5\",\"val_6\",\"val_7\",\"val_8\",\"val_9\"])\n",
    "#train_splits[\"train_0\"] = train_idx[0]\n",
    "#val_splits[\"val_0\"] = val_idx[0]\n",
    "#train_splits[\"train_1\"] = train_idx[1]\n",
    "#val_splits[\"val_1\"] = val_idx[1]\n",
    "#train_splits[\"train_2\"] = train_idx[2]\n",
    "#val_splits[\"val_2\"] = val_idx[2]\n",
    "#train_splits[\"train_3\"] = train_idx[3]\n",
    "#val_splits[\"val_3\"] = val_idx[3]\n",
    "#train_splits[\"train_4\"] = train_idx[4]\n",
    "#val_splits[\"val_4\"] = val_idx[4]\n",
    "#train_splits[\"train_5\"] = train_idx[5]\n",
    "#val_splits[\"val_5\"] = val_idx[5]\n",
    "#train_splits[\"train_6\"] = train_idx[6]\n",
    "#val_splits[\"val_6\"] = val_idx[6]\n",
    "#train_splits[\"train_7\"] = train_idx[7]\n",
    "#val_splits[\"val_7\"] = val_idx[7]\n",
    "#train_splits[\"train_8\"] = train_idx[8]\n",
    "#val_splits[\"val_8\"] = val_idx[8]\n",
    "#train_splits[\"train_9\"] = train_idx[9]\n",
    "#val_splits[\"val_9\"] = val_idx[9]\n",
    "#train_splits.to_csv(\"train_splits.csv\") # save the values in a csv\n",
    "#val_splits.to_csv(\"val_splits.csv\") # save the values in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a424b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering train and validation splits\n",
    "train_idx = pd.read_csv(\"train_splits.csv\")\n",
    "val_idx = pd.read_csv(\"val_splits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542b396",
   "metadata": {},
   "source": [
    "# Images inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed to allow reproducibility\n",
    "random.seed(42) \n",
    "\n",
    "# Getting a random image path\n",
    "random_image_path = random.choice(df.filename)\n",
    "\n",
    "# Opening the image\n",
    "img = Image.open(\"dataset/\"+random_image_path)\n",
    "\n",
    "# Printing metadata\n",
    "print(\"Random image path: \",random_image_path)\n",
    "print(\"\\nImage class: \",df[df.filename == random_image_path].label.item())\n",
    "print(\"\\nImage height: \",img.height) \n",
    "print(\"\\nImage width: \",img.width)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plotting the image with matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_as_array)\n",
    "title_str = \"Image class: \"+str(df[df.filename == random_image_path].label.item())+\"\\nImage shape: \"+str(img_as_array.shape)+\"\\n-> (height, width, channels)\"\n",
    "plt.title(title_str)\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These lines of code take a while\n",
    "\n",
    "# Finding all the unique shapes of the images inside the dataset\n",
    "shape_arr = []\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    with Image.open(\"dataset/\"+df.iloc[i].filename) as img:\n",
    "        width,height = img.size\n",
    "        shape_arr.append((width,height))\n",
    "\n",
    "print(\"The unique shapes of the images in the dataset are: \",np.unique(shape_arr,axis= 0).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The biggest shape is: \",np.max(shape_arr,axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The smallest shape is: \",np.min(shape_arr, axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following lines of code take a lot of time; uncomment to test\n",
    "#\n",
    "## Retrieving the overall mean for subsequent normalization purposes\n",
    "#r_mean_arr = []\n",
    "#g_mean_arr = []\n",
    "#b_mean_arr = []\n",
    "#\n",
    "#for i in range(0,df.shape[0]):\n",
    "#    img_path = \"dataset/\"+df.iloc[i].filename\n",
    "#    with Image.open(img_path) as img:\n",
    "#        img_np = np.array(img.getdata()).reshape(img.size[0],img.size[1],3)\n",
    "#        r_mean,g_mean,b_mean = np.mean(img_np,axis=(0,1))\n",
    "#        r_mean_arr.append(r_mean)\n",
    "#        g_mean_arr.append(g_mean)\n",
    "#        b_mean_arr.append(b_mean)\n",
    "#\n",
    "#R_MEAN = np.mean(r_mean_arr) / 255\n",
    "#G_MEAN = np.mean(g_mean_arr) / 255\n",
    "#B_MEAN = np.mean(b_mean_arr) / 255\n",
    "#\n",
    "#RGB_df = pd.DataFrame(columns= [\"R_MEAN\",\"G_MEAN\",\"B_MEAN\"])\n",
    "#RGB_df[\"R_MEAN\"] = [R_MEAN]\n",
    "#RGB_df[\"G_MEAN\"] = [G_MEAN]\n",
    "#RGB_df[\"B_MEAN\"] = [B_MEAN]\n",
    "#RGB_df.to_csv(\"RGB_df.csv\") # save the values in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b604d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the overall mean from csv\n",
    "RGB_df = pd.read_csv(\"RGB_df.csv\")\n",
    "print(\"Red ch mean = \",RGB_df.iloc[0].R_MEAN.item(),\"\\nGreen ch mean = \",RGB_df.iloc[0].G_MEAN.item(),\"\\nBlue ch mean = \",RGB_df.iloc[0].B_MEAN.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4342c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following lines of code take a lot of time; uncomment to test\n",
    "##\n",
    "## Retrieving the overall standard deviation for subsequent normalization purposes\n",
    "#r_std_arr = []\n",
    "#g_std_arr = []\n",
    "#b_std_arr = []\n",
    "#\n",
    "#for i in range(0,df.shape[0]):\n",
    "#    img_path = \"dataset/\"+df.iloc[i].filename    \n",
    "#    with Image.open(img_path) as img:\n",
    "#        img_np = np.array(img.getdata()).reshape(img.size[0],img.size[1],3)\n",
    "#        r_std,g_std,b_std = np.std(img_np,axis=(0,1))\n",
    "#        r_std_arr.append(r_std)\n",
    "#        g_std_arr.append(g_std)\n",
    "#        b_std_arr.append(b_std)\n",
    "#\n",
    "#R_STD = np.mean(r_std_arr) / 255\n",
    "#G_STD = np.mean(g_std_arr) / 255\n",
    "#B_STD = np.mean(b_std_arr) / 255\n",
    "#\n",
    "#RGB_std_df = pd.DataFrame(columns= [\"R_STD\",\"G_STD\",\"B_STD\"])\n",
    "#RGB_std_df[\"R_STD\"] = [R_STD]\n",
    "#RGB_std_df[\"G_STD\"] = [G_STD]\n",
    "#RGB_std_df[\"B_STD\"] = [B_STD]\n",
    "#RGB_std_df.to_csv(\"RGB_std_df.csv\") # save the values in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the overall standard deviation from csv\n",
    "RGB_std_df = pd.read_csv(\"RGB_std_df.csv\")\n",
    "print(\"Red ch mean = \",RGB_std_df.iloc[0].R_STD.item(),\"\\nGreen ch mean = \",RGB_std_df.iloc[0].G_STD.item(),\"\\nBlue ch mean = \",RGB_std_df.iloc[0].B_STD.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145e6a7",
   "metadata": {},
   "source": [
    "# Dataset standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b04e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transformations on images\n",
    "\n",
    "IMAGE_WIDTH=224\n",
    "IMAGE_HEIGHT=224\n",
    "IMAGE_SIZE=(IMAGE_WIDTH,IMAGE_HEIGHT)\n",
    "R_MEAN = RGB_df.iloc[0].R_MEAN.item()\n",
    "G_MEAN = RGB_df.iloc[0].G_MEAN.item()\n",
    "B_MEAN = RGB_df.iloc[0].B_MEAN.item()\n",
    "R_STD = RGB_std_df.iloc[0].R_STD.item()\n",
    "G_STD = RGB_std_df.iloc[0].G_STD.item()\n",
    "B_STD = RGB_std_df.iloc[0].B_STD.item()\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Resizing the images to IMAGE_SIZE x IMAGE_SIZE \n",
    "    transforms.Resize(size=IMAGE_SIZE),\n",
    "    # Turning the image into a torch.Tensor\n",
    "    transforms.ToTensor(), # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "    # Normalizing to the overall mean and std\n",
    "    transforms.Normalize(mean=[R_MEAN,G_MEAN,B_MEAN],std=[R_STD,G_STD,B_STD])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcef682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of transformation on one image of example\n",
    "\n",
    "# Setting the seed to allow reproducibility\n",
    "random.seed(42) \n",
    "\n",
    "# Getting a random image path\n",
    "random_image_path = random.choice(df.filename)\n",
    "\n",
    "# Opening the image\n",
    "img = Image.open(\"dataset/\"+random_image_path)\n",
    "\n",
    "# Plotting the original image\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img)\n",
    "title_str = \"Original \\nSize: \"+str(img.size)\n",
    "ax[0].set_title(title_str)\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "# Transform and plot image\n",
    "# Note: permute() will change shape of image to suit matplotlib \n",
    "# (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "transformed_image = data_transform(img).permute(1, 2, 0) \n",
    "ax[1].imshow(transformed_image) \n",
    "title_str = \"Transformed \\nSize: \"+str(transformed_image.shape)\n",
    "ax[1].set_title(title_str)\n",
    "ax[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae68d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom Dataset\n",
    "class DogsCatsData(Dataset):\n",
    "    def __init__(self,x,y,transform=None):\n",
    "        self.x = x.reset_index()\n",
    "        self.y = y.reset_index()\n",
    "        self.transform = transform        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def load_image(self,path):\n",
    "        prefix = \"dataset/\"\n",
    "        return Image.open(os.path.join(prefix,path))\n",
    "    def __getitem__(self,index):\n",
    "        image = self.load_image(self.x.iloc[index].filename)\n",
    "        label = self.y.iloc[index].label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {\"image\":image,\"label\":label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df56c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping the first split of the 10-folds cross-validation\n",
    "## x and y of the new DataSet correspond to filenames and labels, respectively\n",
    "train_data = DogsCatsData(df.iloc[train_idx[\"train_0\"].values][\"filename\"],df.iloc[train_idx[\"train_0\"].values][\"label\"],transform=data_transform)\n",
    "val_data = DogsCatsData(df.iloc[val_idx[\"val_0\"].values][\"filename\"],df.iloc[val_idx[\"val_0\"].values][\"label\"],transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths\n",
    "print(\"The lengths of the training and validation sets: \",len(train_data),len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722e6f5",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c016dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn train and validation Datasets into DataLoaders\n",
    "trainloader = DataLoader(train_data,batch_size=32)\n",
    "validationloader = DataLoader(val_data,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03864841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the content of the first training batch\n",
    "for i_batch,sample_batched in enumerate(trainloader):\n",
    "    if i_batch == 1:\n",
    "        break\n",
    "    fig,axs = plt.subplots(ncols=8,nrows=4,figsize=(15, 8))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(len(sample_batched[\"image\"])):\n",
    "        axs[i].imshow(sample_batched[\"image\"][i].permute(1,2,0))\n",
    "        axs[i].set_title(str(sample_batched[\"label\"][i]))\n",
    "        axs[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d157ed",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6048e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN-based image classifier.\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2)) \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=512*3*3, out_features=2))\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiating an object.\n",
    "model = ImageClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1673efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a single image from the batch\n",
    "for i_batch,sample_batched in enumerate(trainloader):\n",
    "    if i_batch == 1:\n",
    "        break\n",
    "    img_single = sample_batched[\"image\"][0].unsqueeze(dim=0)\n",
    "    label_single = sample_batched[\"label\"][0]\n",
    "print(\"Single image shape: \",img_single.shape,\"\\n\")\n",
    "\n",
    "# Performing a forward pass on a single image\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(img_single.to(device))\n",
    "    \n",
    "# Printing out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(\"Output logits:\\n\",pred,\"\\n\")\n",
    "print(\"Output prediction probabilities:\\n\",torch.softmax(pred, dim=1),\"\\n\")\n",
    "print(\"Output prediction label:\\n\",torch.argmax(torch.softmax(pred, dim=1), dim=1),\"\\n\")\n",
    "print(\"Actual label:\\n\",label_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a test pass through of an example input size \n",
    "summary(model,input_size=[1,3,IMAGE_WIDTH,IMAGE_HEIGHT]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e333e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the operations to do in a training step\n",
    "def train_step(model:torch.nn.Module, \n",
    "               dataloader:torch.utils.data.DataLoader, \n",
    "               loss_fn:torch.nn.Module, \n",
    "               optimizer:torch.optim.Optimizer):    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss,train_acc = 0,0\n",
    "    # Loop through DataLoader batches\n",
    "    for batch,sample_batched in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X = sample_batched[\"image\"].to(device)\n",
    "        y = sample_batched[\"label\"].to(device)\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        # Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss += loss.item() \n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the operations to do in a validation step\n",
    "def val_step(model:torch.nn.Module, \n",
    "              dataloader:torch.utils.data.DataLoader, \n",
    "              loss_fn:torch.nn.Module):    \n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    # Setup validation loss and validation accuracy values\n",
    "    val_loss,val_acc = 0,0\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch,sample_batched in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X = sample_batched[\"image\"].to(device)\n",
    "            y = sample_batched[\"label\"].to(device)            \n",
    "            # Forward pass\n",
    "            val_pred_logits = model(X)\n",
    "            # Calculate and accumulate loss\n",
    "            loss = loss_fn(val_pred_logits, y)\n",
    "            val_loss += loss.item()\n",
    "            # Calculate and accumulate accuracy\n",
    "            val_pred_labels = val_pred_logits.argmax(dim=1)\n",
    "            val_acc += ((val_pred_labels == y).sum().item()/len(val_pred_labels))\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    val_acc = val_acc / len(dataloader)\n",
    "    return val_loss,val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking in various parameters required for training and validation steps\n",
    "def train(model:torch.nn.Module, \n",
    "          train_dataloader:torch.utils.data.DataLoader, \n",
    "          val_dataloader:torch.utils.data.DataLoader, \n",
    "          optimizer:torch.optim.Optimizer,\n",
    "          loss_fn:torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs:int = 5,\n",
    "          split:int = 0):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": []\n",
    "    }\n",
    "    # Instantiating the best validation accuracy\n",
    "    best_val = 0\n",
    "    # Loop through training and validation steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        val_loss, val_acc = val_step(model=model,\n",
    "            dataloader=val_dataloader,\n",
    "            loss_fn=loss_fn) \n",
    "        # Saving the model obtaining the best validation accuracy through the epochs\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            checkpoint = {\"model\": ImageClassifier(),\n",
    "                          \"state_dict\": model.state_dict(),\n",
    "                          \"optimizer\": optimizer.state_dict()}\n",
    "            checkpoint_name = \"checkpoint_\"+str(split)+\".pth\"\n",
    "            torch.save(checkpoint, checkpoint_name)    \n",
    "        else:\n",
    "            continue\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \"\n",
    "            f\"val_acc: {val_acc:.4f}\"\n",
    "        )\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d1c43",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f126c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These lines take a lot of time; you will load the trained model in the following \n",
    "#\n",
    "## Set random seeds\n",
    "#torch.manual_seed(42) \n",
    "#torch.cuda.manual_seed(42)\n",
    "#\n",
    "## Set number of epochs\n",
    "#NUM_EPOCHS = 50\n",
    "#\n",
    "## Setup loss function and optimizer\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "#\n",
    "## Start the timer\n",
    "#from timeit import default_timer as timer \n",
    "#start_time = timer()\n",
    "#\n",
    "## Train model_0 \n",
    "#model_results = train(model=model,\n",
    "#                      train_dataloader=trainloader,\n",
    "#                      val_dataloader=validationloader,\n",
    "#                      optimizer=optimizer,\n",
    "#                      loss_fn=loss_fn,\n",
    "#                      epochs=NUM_EPOCHS,\n",
    "#                      split=0)\n",
    "#\n",
    "## End the timer and print out how long it took\n",
    "#end_time = timer()\n",
    "#print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974755f",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are the results from the previous step; you will load the results in the following\n",
    "#\n",
    "## Extract train and validation loss and accuracy at each epoch \n",
    "#results = dict(list(model_results.items()))\n",
    "#\n",
    "## Get the loss values of the results dictionary (training and validation)\n",
    "#train_loss = results[\"train_loss\"]\n",
    "#val_loss = results[\"val_loss\"]\n",
    "#\n",
    "## Get the accuracy values of the results dictionary (training and validation)\n",
    "#train_acc = results[\"train_acc\"]\n",
    "#val_acc = results[\"val_acc\"]\n",
    "#\n",
    "## Figure out how many epochs there were\n",
    "#epochs = range(len(results[\"train_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results in a csv\n",
    "#results_df = pd.DataFrame(columns= [\"train_loss\",\"val_loss\",\"train_acc\",\"val_acc\",\"epochs\"])\n",
    "#results_df[\"train_loss\"] = train_loss\n",
    "#results_df[\"val_loss\"] = val_loss\n",
    "#results_df[\"train_acc\"] = train_acc\n",
    "#results_df[\"val_acc\"] = val_acc\n",
    "#results_df[\"epochs\"] = epochs\n",
    "#results_df_name = \"results_df_\"+str(0)+\".csv\"\n",
    "#results_df.to_csv(results_df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58a4336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and validation results from csv\n",
    "results_from_csv = pd.read_csv(\"results_df_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025b1a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup a plot \n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(results_from_csv[\"epochs\"],results_from_csv[\"train_loss\"],label=\"train_loss\")\n",
    "plt.plot(results_from_csv[\"epochs\"],results_from_csv[\"val_loss\"],label=\"val_loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(results_from_csv[\"epochs\"],results_from_csv[\"train_acc\"],label=\"train_accuracy\")\n",
    "plt.plot(results_from_csv[\"epochs\"],results_from_csv[\"val_acc\"],label=\"val_accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d79ba",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can find the model and result corresponding to each fold in the files inside the directory\n",
    "#\n",
    "## Continue the training and validation of the model for all the other folds\n",
    "#for i in range(1,10):\n",
    "#    # Keeping the split\n",
    "#    train_str = \"train_\"+str(i)\n",
    "#    val_str = \"val_\"+str(i)\n",
    "#    train_data = DogsCatsData(df.iloc[train_idx[train_str].values][\"filename\"],df.iloc[train_idx[train_str].values][\"label\"],transform=data_transform)\n",
    "#    val_data = DogsCatsData(df.iloc[val_idx[val_str].values][\"filename\"],df.iloc[val_idx[val_str].values][\"label\"],transform=data_transform)\n",
    "#    # Turn train and validation Datasets into DataLoaders\n",
    "#    trainloader = DataLoader(train_data,batch_size=32)\n",
    "#    validationloader = DataLoader(val_data,batch_size=32)\n",
    "#    # Start the timer\n",
    "#    from timeit import default_timer as timer \n",
    "#    start_time = timer()\n",
    "#    # Train model \n",
    "#    model_results = train(model=model,\n",
    "#                          train_dataloader=trainloader,\n",
    "#                          val_dataloader=validationloader,\n",
    "#                          optimizer=optimizer,\n",
    "#                          loss_fn=loss_fn,\n",
    "#                          epochs=NUM_EPOCHS,\n",
    "#                          split=i)\n",
    "#    # End the timer and print out how long it took\n",
    "#    end_time = timer()\n",
    "#    print(f\"Total training time for split {i}: {end_time-start_time:.3f} seconds\")\n",
    "#    # Extract train and validation loss and accuracy at each epoch \n",
    "#    results = dict(list(model_results.items()))\n",
    "#    # Get the loss values of the results dictionary (training and validation)\n",
    "#    train_loss = results[\"train_loss\"]\n",
    "#    val_loss = results[\"val_loss\"]\n",
    "#    # Get the accuracy values of the results dictionary (training and validation)\n",
    "#    train_acc = results[\"train_acc\"]\n",
    "#    val_acc = results[\"val_acc\"]\n",
    "#    # Figure out how many epochs there were\n",
    "#    epochs = range(len(results[\"train_loss\"]))\n",
    "#    # Save results in a csv\n",
    "#    results_df = pd.DataFrame(columns= [\"train_loss\",\"val_loss\",\"train_acc\",\"val_acc\",\"epochs\"])\n",
    "#    results_df[\"train_loss\"] = train_loss\n",
    "#    results_df[\"val_loss\"] = val_loss\n",
    "#    results_df[\"train_acc\"] = train_acc\n",
    "#    results_df[\"val_acc\"] = val_acc\n",
    "#    results_df[\"epochs\"] = epochs\n",
    "#    results_df_name = \"results_df_\"+str(i)+\".csv\"\n",
    "#    results_df.to_csv(results_df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9a34d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping the best model\n",
    "\n",
    "# Looking across all the best validation accuracies obtained from the ten folds\n",
    "val_accuracies = np.zeros([10,1])\n",
    "for i in range(10):\n",
    "    results_string = \"results_df_\"+str(i)+\".csv\"\n",
    "    val_accuracies[i] = np.max(pd.read_csv(results_string)[\"val_acc\"])\n",
    "    \n",
    "# Get the fold corresponding to the overall best\n",
    "index = np.argmax(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2019c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the checkpoint\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath,map_location=torch.device(\"cpu\")) \n",
    "    model = checkpoint[\"model\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_string = \"checkpoint_\"+str(index)+\".pth\"\n",
    "model_cp = load_checkpoint(model_string)\n",
    "print(model_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ea044",
   "metadata": {},
   "source": [
    "# Explainability through Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best fold\n",
    "val_string = \"val_\"+str(index)\n",
    "\n",
    "# Getting a random image path from validation\n",
    "test_image_path = \"dog.11212.jpg\" \n",
    "test_image_label = 1\n",
    "\n",
    "# Opening the image\n",
    "img = Image.open(\"dataset/\"+test_image_path)\n",
    "test_image = data_transform(img).unsqueeze(dim=0) \n",
    "plt.imshow(data_transform(img).permute(1, 2, 0)) \n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fa297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a forward pass on a single image\n",
    "model_cp.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_cp(test_image.to(device))\n",
    "    \n",
    "# Printing out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(\"Output logits:\\n\",pred,\"\\n\")\n",
    "print(\"Output prediction probabilities:\\n\",torch.softmax(pred, dim=1),\"\\n\")\n",
    "print(\"Output prediction label:\\n\",torch.argmax(torch.softmax(pred, dim=1), dim=1),\"\\n\")\n",
    "print(\"Actual label:\\n\",test_image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4cbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occlude a part of the image\n",
    "test_image_occ = data_transform(img)\n",
    "test_image_occ[:,24:72,60:108] = 128\n",
    "plt.imshow(test_image_occ.permute(1,2,0)) \n",
    "plt.axis(\"off\")\n",
    "test_image_occ = test_image_occ.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e04c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test again on the occluded image\n",
    "model_cp.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_cp(test_image_occ.to(device))\n",
    "    \n",
    "# Printing out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(\"Output logits:\\n\",pred,\"\\n\")\n",
    "print(\"Output prediction probabilities:\\n\",torch.softmax(pred,dim=1),\"\\n\")\n",
    "print(\"Output prediction label:\\n\",torch.argmax(torch.softmax(pred,dim=1),dim=1),\"\\n\")\n",
    "print(\"Actual label:\\n\",test_image_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47b8a7",
   "metadata": {},
   "source": [
    "Now the dog is classified as a cat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85a2bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a systematic occlusion\n",
    "\n",
    "# Defining the mask \n",
    "MASK_WIDTH = 48\n",
    "MASK_HEIGHT = 48\n",
    "\n",
    "# Deriving the coordinates of the mask center on the image\n",
    "X = np.arange(IMAGE_WIDTH)\n",
    "Y = np.arange(IMAGE_HEIGHT)\n",
    "X1,Y1 = np.meshgrid(X,Y,indexing=\"xy\")\n",
    "\n",
    "# Deriving all the starting X and Y of the mask on the image\n",
    "X1 = X1.flatten(\"F\") - MASK_WIDTH/2\n",
    "Y1 = Y1.flatten(\"F\") - MASK_HEIGHT/2\n",
    "\n",
    "# Deriving all the ending X and Y of the mask on the image\n",
    "X2 = X1 + MASK_WIDTH\n",
    "Y2 = Y1 + MASK_HEIGHT\n",
    "\n",
    "# Convert type for subsequent indexing\n",
    "X1 = X1.astype(int)\n",
    "X2 = X2.astype(int)\n",
    "Y1 = Y1.astype(int)\n",
    "Y2 = Y2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulating all X and Y in order to avoid point outside the image\n",
    "X1 = np.maximum(0,X1)\n",
    "Y1 = np.maximum(0,Y1)\n",
    "X2 = np.minimum(IMAGE_WIDTH,X2)\n",
    "Y2 = np.minimum(IMAGE_HEIGHT,Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f95ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These lines of code take a lot of time\n",
    "\n",
    "# Performing predictions on all the possible occluded version of the image\n",
    "\n",
    "# TO DO   ->   IMPLEMENTE HERE THE OCCLUSION ALGORITHM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading predictions from csv\n",
    "pred_df = pd.read_csv(\"pred_df.csv\")\n",
    "\n",
    "# Visualize the image \n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax1 = fig.add_subplot(121)\n",
    "im1 = ax1.imshow(data_transform(img).permute(1,2,0))\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# Visualize the predicted probability of being either class 0 or 1 (where 1 is 'dog') \n",
    "pred_mask = np.reshape(pred_df[\"class_1\"].to_numpy(),(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "ax2 = fig.add_subplot(122)\n",
    "im2 = ax2.imshow(pred_mask,cmap=plt.cm.hot)\n",
    "ax2.axis(\"off\")\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im2, cax=cax, orientation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd3f7",
   "metadata": {},
   "source": [
    "The relevance is high at low values predicted probability because if these parts of image are masked, then the image is erroneously classified as cat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize which parts of the image are more relevant for a correct classification\n",
    "thresh_pred = pred_mask < 0.35  \n",
    "plt.imshow(data_transform(img).permute(1,2,0) * thresh_pred[...,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83133f",
   "metadata": {},
   "source": [
    "It seems that the dog ear and body shape have a high relevance for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7dd99",
   "metadata": {},
   "source": [
    "## Explainability through Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform image for model classification\n",
    "test_image_sal = data_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "# Gradient of higest score are required to be calculated for the tensor image \n",
    "# TO DO -> set the gradients of the input image true\n",
    "    \n",
    "\n",
    "# Gradients are not needed for the model parameters\n",
    "# TO DO -> set the model parameters in a state where the gradients are not computed\n",
    "    \n",
    "\n",
    "# Set model in eval mode\n",
    "# TO DO -> set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Forward pass to calculate predictions\n",
    "# TO DO -> compute the prediction from the model using the test_image_sal as input\n",
    "# TO DO -> extract the max score and corresponding index from the prediction\n",
    "\n",
    "\n",
    "# Backward pass to get gradients of scores predicted classes w.r.t. input image\n",
    "# TO DO -> compute the backward function of the output score\n",
    "    \n",
    "# Get max along channel axis\n",
    "saliency,_ = torch.max(torch.abs(test_image_sal.grad[0]),dim=0)\n",
    "# Normalize to [0 ... 1]\n",
    "saliency = (saliency - saliency.min())/(saliency.max()-saliency.min())\n",
    "\n",
    "# Visualize the image \n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax1 = fig.add_subplot(121)\n",
    "im1 = ax1.imshow(data_transform(img).permute(1,2,0))\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# Visualize the saliency map \n",
    "ax2 = fig.add_subplot(122)\n",
    "im2 = ax2.imshow(saliency.numpy(),cmap=plt.cm.hot)\n",
    "ax2.axis(\"off\")\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im2, cax=cax, orientation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda4f6c",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c35730",
   "metadata": {},
   "source": [
    "Python package for doing XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These lines take a lot of time as when built from scratch\n",
    "#\n",
    "#from captum.attr import Occlusion\n",
    "#\n",
    "## Defining Occlusion interpreter\n",
    "#ablator = Occlusion(model_cp)\n",
    "## Computes occlusion attribution, ablating each patch,\n",
    "## shifting in each direction by the default of 1.\n",
    "#occ_attr = ablator.attribute(data_transform(img).unsqueeze(dim=0), target=1, sliding_window_shapes=(3,48,48))\n",
    "#\n",
    "## Save to file\n",
    "#torch.save(occ_attr, \"occ_attr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the attributions\n",
    "occ_attr = torch.load(\"occ_attr.pt\",map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO -> import Saliency model from Captum\n",
    "\n",
    "# Defining Saliency interpreter\n",
    "# TO DO -> define Saliency\n",
    "\n",
    "# Computes saliency maps for class 1\n",
    "# TO DO -> compute Saliency w.r.t. class 1\n",
    "sal_attr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d88a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attribution maps \n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax1 = fig.add_subplot(121)\n",
    "im1 = ax1.imshow(np.transpose(occ_attr.squeeze().cpu().detach().numpy(), (1,2,0)),cmap=plt.cm.hot)\n",
    "ax1.set_title(\"Occlusion\")\n",
    "ax1.axis(\"off\")\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "im2 = ax2.imshow(np.transpose(sal_attr.squeeze().cpu().detach().numpy(), (1,2,0)),cmap=plt.cm.hot)\n",
    "ax2.set_title(\"Saliency\")\n",
    "ax2.axis(\"off\")\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im2, cax=cax, orientation='vertical')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
